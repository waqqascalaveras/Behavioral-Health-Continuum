# Setup & Usage Guide

## Installation

### 1. Clone or Download the Project
```bash
git clone <repository>
cd behavioral_health_etl
```

### 2. Create Python Virtual Environment (Recommended)
```powershell
# Create virtual environment
python -m venv .venv

# Activate it
.venv\Scripts\Activate.ps1  # PowerShell
# or
.venv\Scripts\activate.bat  # Command Prompt
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

**Key dependencies:**
- pandas â€” Data processing
- pandera â€” Schema validation
- plotly â€” Interactive visualizations
- streamlit â€” Dashboard framework
- openpyxl â€” Excel export
- pyarrow â€” Parquet format
- geopandas â€” Geospatial analysis

## Quick Start

### Option 1: Double-Click to Launch (Easiest)
Just double-click any of these files:
- **`run_dashboard.py`** â€” View the dashboard immediately
- **`run_etl.py`** â€” Process and export data
- **`launcher.py`** â€” Show menu with all options

### Option 2: Command Line
```powershell
# Launch interactive dashboard
python run_dashboard.py

# Process data with ETL pipeline
python run_etl.py

# Show menu with all options
python launcher.py
```

## Running the Dashboard

**Easiest way:**
```powershell
python run_dashboard.py
```

This script will:
âœ… Set up the Python path
âœ… Launch Streamlit server
âœ… Open browser automatically to http://localhost:8501
âœ… Display all 5 interactive dashboards

**Manual way:**
```powershell
python -m streamlit run dashboards.py
```

### Dashboard Pages
1. ğŸ¥ **Providers** â€” Medi-Cal FFS providers in Calaveras County
2. ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ **Foster Care** â€” Child welfare outcomes and metrics
3. ğŸ“ˆ **Census** â€” Population demographics (ACS 5-Year)
4. âš–ï¸ **Grievances & Appeals** â€” Behavioral health service complaints
5. ğŸ§  **Depression** â€” Adult depression prevalence trends

## Running the ETL Pipeline

**Easiest way:**
```powershell
python run_etl.py
```

This will:
âœ… Load all configured datasets
âœ… Validate data schemas
âœ… Filter to Calaveras County (where applicable)
âœ… Export to CSV, Parquet, SQLite, and Excel

**Manual way:**
```powershell
python -m etl.main
```

### Output Files
After running ETL, you'll find:
- `output_csv/` â€” CSV files for each dataset
- `output_parquet/` â€” Parquet files (for big data tools)
- `output_data.sqlite` â€” SQLite database
- `behavioral_health_dashboard_data.xlsx` â€” Excel workbook for Tableau

## Configuration

### Adding New Data Sources

**Edit:** `etl/config.py`
```python
DATA_SOURCES = {
    'my_dataset': 'path/to/file.csv',
    # or
    'my_dataset': 'https://example.com/data.csv',
}
```

### Customizing Data Processing

**Edit:** `etl/process_clean.py`
1. Add schema validation in `_schemas()` function
2. Add county filtering in `_filter_by_county()` function
3. Add custom cleaning logic for your dataset

### Adding Dashboard Pages

**Edit:** `dashboards.py`
1. Create new function `def show_my_dashboard():`
2. Add to navigation sidebar
3. Load and visualize your data

## Troubleshooting

### "ModuleNotFoundError: No module named 'streamlit'"
**Fix:** Install dependencies
```bash
pip install -r requirements.txt
```

### "Port 8501 is already in use"
**Fix:** The dashboard is already running. Either:
- Stop the existing dashboard (Ctrl+C)
- Or use a different port:
```powershell
python -m streamlit run dashboards.py --server.port=8502
```

### "Python not found"
**Fix:** Make sure Python is installed and in your PATH
```powershell
python --version
```

### ETL Pipeline Fails
**Fix:** Check the logs
```powershell
# Run with detailed output
python -m etl.main
```

## Data Sources

### Calaveras County Data
1. **Medi-Cal Providers** â€” FFS provider directory
2. **Foster Care** â€” CFSR4 outcomes data
3. **Census** â€” ACS 5-Year demographic estimates
4. **ABGAR** â€” Behavioral health grievances and appeals (DHCS)

### State-Level Data
5. **Depression Prevalence** â€” BRFSS survey (CDPH)

### Adding More Data
See `CHHS_DATASET_RECOMMENDATIONS.md` for analysis of 407 available CHHS datasets

## Project Structure

```
behavioral_health_etl/
â”œâ”€â”€ run_dashboard.py          # Launch dashboard
â”œâ”€â”€ run_etl.py                # Run ETL pipeline
â”œâ”€â”€ launcher.py               # Interactive menu
â”œâ”€â”€ dashboards.py             # Streamlit dashboard code
â”œâ”€â”€ etl/                       # ETL pipeline
â”‚   â”œâ”€â”€ main.py              # Entry point
â”‚   â”œâ”€â”€ config.py            # Data source configuration
â”‚   â”œâ”€â”€ download.py          # Load data
â”‚   â”œâ”€â”€ process_clean.py     # Validate and clean
â”‚   â”œâ”€â”€ export.py            # Export formats
â”‚   â”œâ”€â”€ logger.py            # Logging
â”‚   â””â”€â”€ downloads/           # Input data
â”œâ”€â”€ src/                       # Supporting modules
â”‚   â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ output_csv/              # CSV exports
â”œâ”€â”€ output_parquet/          # Parquet exports
â””â”€â”€ README.md                # This file
```

## Development

### Running Tests
```bash
# Run Python syntax check
python -m py_compile *.py etl/*.py

# Run data validation tests
python -m pytest tests/
```

### Code Style
- Follow PEP 8
- Use type hints where possible
- Document functions with docstrings

### Adding Datasets
1. Place data in `etl/downloads/dataset_name/`
2. Add to `etl/config.py` DATA_SOURCES dict
3. Add schema to `etl/process_clean.py`
4. Run ETL pipeline: `python run_etl.py`
5. Create dashboard visualization if desired

## Getting Help

1. **Check logs:** Look in `etl.log` for error details
2. **Review code:** Docstrings explain each module
3. **Read README:** [README.md](README.md) has technical details
4. **Check scripts:** All launcher scripts have helpful comments

## Common Tasks

### Export data to different formats
The ETL pipeline automatically exports to:
- CSV (spreadsheets, easy sharing)
- Parquet (big data tools)
- SQLite (databases, queries)
- Excel (Tableau, reporting)

### Create new dashboard page
1. Write function in `dashboards.py`
2. Add to navigation radio buttons
3. Use Streamlit (`st.*`) commands for UI
4. Use Plotly for interactive charts

### Add new dataset
1. Download data to `etl/downloads/my_data/`
2. Update `etl/config.py`
3. Add schema to `etl/process_clean.py`
4. Run ETL: `python run_etl.py`
5. Access via CSV, Parquet, SQLite, or Excel

## Performance Notes

- **CSV exports:** Fast, human-readable, for spreadsheets
- **Parquet exports:** Compressed, efficient, for big data
- **SQLite:** Queryable database, good for analysis
- **Excel:** Good for Tableau, slower for very large datasets

## Security Notes

- Store sensitive URLs in environment variables
- Don't commit API keys to version control
- Use `.gitignore` to exclude output directories
- Keep `etl/downloads/` with git if data is non-sensitive

---

**Happy analyzing! ğŸ“Š**

For more details, see [README.md](README.md)
